% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/EBcoBART_Functions.R
\name{EBcoBART}
\alias{EBcoBART}
\title{Learning prior covariate weights for BART models using empirical Bayes and co-data.}
\usage{
EBcoBART(
  Y,
  X,
  CoData,
  model,
  nIter = 10,
  EB = F,
  Prob_Init = c(rep(1/ncol(X), ncol(X))),
  Info = F,
  Seed = T,
  ndpost = 5000,
  nskip = 5000,
  nchain = 5,
  keepevery = 1,
  ntree = 50,
  alpha = 0.95,
  beta = 2,
  k = 2,
  sigest = sd(Y) * 0.667,
  sigdf = 10,
  sigquant = 0.75
)
}
\arguments{
\item{Y}{Response variable that can be either continuous or binary. Should be a numeric.}

\item{X}{Explanatory variables. Should be a matrix. If X is a data.frame and contains
factors, you may consider the function Dat_EBcoBART}

\item{CoData}{The co-data model matrix with co-data information on explanatory variables in X.
Should be a matrix, so not a data.frame.
If grouping information is present, please encode this yourself using dummies
with dummies representing which group a explanatory variable belongs to.
The number of rows of the co-data matrix should equal the number of columns of X}

\item{model}{What type of response variable Y. Can be either continuous or binary}

\item{nIter}{Number of iterations of the EM algorithm}

\item{EB}{Logical (T/F). If true the EM algorithm also estimates prior parameters
alpha (of tree structure prior) and k (of leaf node parameter prior). Defaults to False.
Setting to true increases computational time.}

\item{Prob_Init}{Initial vector of splitting probabilities for explanatory variables X.
Lenght should equal number of columns of X (and number of rows in CoData).
Defaults to 1/p, i.e. equal weight for each variable.}

\item{Info}{Logical. Asks whether information about the algorithm progress should be printed.
Defaults to FALSE.}

\item{Seed}{Logical asking whether a seed should be set for reproduciblity. Defaults to TRUE.}

\item{ndpost}{Number of posterior samples returned by dbarts after burn-in. Same as in dbarts.
Defaults to 5000.}

\item{nskip}{Number of burn-in samples. Same as in dbarts. Defaults to 5000.}

\item{nchain}{Number of independent mcmc chains. Same as in dbarts. Defaults to 5.}

\item{keepevery}{Thinning. Same as in dbarts. Defaults to 1.}

\item{ntree}{Number of trees in the BART model. Same as in dbarts. Defaults to 50.}

\item{alpha}{Alpha parameter of tree structure prior. Called base in dbarts. Defaults to 0.95.}

\item{beta}{Beta parameter of tree structure prior. Called power in dbarts. Defaults to 2.}

\item{k}{Parameter for leaf node parameter prior. Same as in dbarts. Defaults to 2.}

\item{sigest}{Only for continuous response. Estimate of error variance
used to set inverse gamma prior on error variance. Same as in dbarts. Defaults to 0.667*var(Y)}

\item{sigdf}{Only for continuous response. Degrees of freedom for error variance prior
Same as in dbarts. Defaults to 10.}

\item{sigquant}{Only for continuous response. Quantile at which sigest is placed Same as in dbarts.
Defaults to 0.75.}
}
\value{
A list object with the estimated variable weights, i.e the probabilities that variables are selected
in the splitting rules. Additionaly, the final co-data model is returned. If EB is set to TRUE, estimates of k and alpha
are also returned.
The prior parameter estimates can then be used in your favorite BART R package that supports
manually setting the splitting variable probability vector (dbarts and BARTMachine).
}
\description{
Function that estimates the prior probabilities of variables getting selected in the splitting rules
of Bayesian Additive Regression Trees (BART). Estimation is performed using empirical Bayes and co-data,
i.e. external information on the explanatory variables.
}
\examples{
###################################
### Continuous response example ###
###################################

# Simulate data from Friedman function and define Co-Data as grouping structure

sigma <- 1.0
N <- 100
p <- 500
G <- 5   #number of groups
CoDat = rep(1:G, rep(p/G,G))
CoDat = data.frame(factor(CoDat))
CoDat <- model.matrix(~0+., CoDat)
colnames(CoDat)  = paste0("Group ",1:G)
g <- function(x) {
 10 * sin(pi * x[,1] * x[,2]) + 20 * (x[,101] - 0.5)^2 + 10 * x[,102] + 10 * x[,3]
}
X <- matrix(runif(N * p), N, p)#'
Y <- g(X)+ rnorm(N, 0, sigma)

Fit <- EBcoBART(Y=Y,X=X,CoData = CoDat, nIter = 15, model = "continuous",
                EB = F, Info = T, Seed = T,
                nchain = 5, nskip = 1000, ndpost = 1000,
                Prob_Init = rep(1/ncol(X),ncol(X)),
                k = 2, alpha = .95, beta = 2)
EstProbs <- Fit$SplittingProbs #estimated prior probabilities of variables getting selected in splitting rules

# The prior parameter estimate EstProbs can then be used
# in your favorite BART fitting package
# We use dbarts:

FinalFit <- dbarts::bart(x.train = X, y.train = Y, # training data
                        ndpost = 5000, # number of posterior samples
                        nskip = 5000,  # number of "warmup" samples to discard
                        nchain = 5,    # number of independent chains
                        ntree = 50,    # number of trees
                        k = 2, base = .95, power = beta, # prior parameters tree
                        sigest = .667*var(Y),sigdf = 10, sigquant = .75,  # prior parameters error variance
                        splitprobs = EstProbs,   # prior variable weights
                        combinechains = T, verbose = F)

###################################
### Binary response example ######
###################################

# Use data set provided in R package
# We set EB=T indicating that we also estimate
# tree structure prior parameter alpha
# and leaf node prior parameter k

data(dat)
Xtr <- as.matrix(dat$Xtrain) # Xtr should be matrix object
Ytr <- dat$Ytrain
Xte <- as.matrix(dat$Xtest) # Xte should be matrix object
Yte <- dat$Ytest
CoDat <- dat$CoData
CoDat <- model.matrix(~0+., CoDat) # encode grouping structure by dummies
remove(dat)

Fit <- EBcoBART(Y=Ytr,X=Xtr,CoData = CoDat, nIter = 15, model = "binary",
                EB = T, Info = T, Seed = T,
                nchain = 5, nskip = 1000, ndpost = 1000,
                Prob_Init = rep(1/ncol(Xtr),ncol(Xtr)),
                k = 2, alpha = .95, beta = 2)
EstProbs <- Fit$SplittingProbs #estimated prior probabilities of variables getting selected in splitting rules
alpha_EB <- Fit$alpha_est
k_EB <- Fit$k_est

# The prior parameter estimates EstProbs, alpha_EB,
# and k_EB can then be used in your favorite BART fitting package
# We use dbarts:

FinalFit <- dbarts::bart(x.train = Xtr, y.train = Ytr, # training data
                         x.test = Xte, # test X for predictions
                         ndpost = 5000,   # number of posterior samples
                         nskip = 5000, # number of "warmup" samples to discard
                         nchain = 5,   # number of independent, parallel chains
                         ntree = 50,    # number of trees
                         k = k_EB, base = alpha_EB, power = 2, # prior parameters tree
                         splitprobs = EstProbs, # prior variable weights
                         combinechains = T, verbose = F)

}
\references{
\CRANpkg{dbarts}

Jerome H. Friedman.
"Multivariate Adaptive Regression Splines."
The Annals of Statistics, 19(1) 1-67 March, 1991.

Hugh A. Chipman, Edward I. George, Robert E. McCulloch.
"BART: Bayesian additive regression trees."
The Annals of Applied Statistics, 4(1) 266-298 March 2010.

Jeroen M. Goedhart, Thomas Klausch, Jurriaan Janssen, Mark A. van de Wiel.
"Co-data Learning for Bayesian Additive Regression Trees."
arXiv preprint arXiv:2311.09997. 2023 Nov 16.
}
\author{
Jeroen M. Goedhart, \email{j.m.goedhart@amsterdamumc.nl}
}
